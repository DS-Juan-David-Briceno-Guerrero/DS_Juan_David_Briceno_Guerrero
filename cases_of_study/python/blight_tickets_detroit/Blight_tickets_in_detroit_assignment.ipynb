{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Understanding and Predicting Property Maintenance Fines\n",
    "\n",
    "This assignment is based on a data challenge from the Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)). \n",
    "\n",
    "The Michigan Data Science Team ([MDST](http://midas.umich.edu/mdst/)) and the Michigan Student Symposium for Interdisciplinary Statistical Sciences ([MSSISS](https://sites.lsa.umich.edu/mssiss/)) have partnered with the City of Detroit to help solve one of the most pressing problems facing Detroit - blight. [Blight violations](http://www.detroitmi.gov/How-Do-I/Report/Blight-Complaint-FAQs) are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid. Enforcing unpaid blight fines is a costly and tedious process, so the city wants to know: how can we increase blight ticket compliance?\n",
    "\n",
    "The first step in answering this question is understanding when and why a resident might fail to comply with a blight ticket. This is where predictive modeling comes in. For this assignment, your task is to predict whether a given blight ticket will be paid on time.\n",
    "\n",
    "All data for this assignment has been provided to us through the [Detroit Open Data Portal](https://data.detroitmi.gov/). **Only the data already included in your Coursera directory can be used for training the model for this assignment.** Nonetheless, we encourage you to look into data from other Detroit datasets to help inform feature creation and model selection. We recommend taking a look at the following related datasets:\n",
    "\n",
    "* [Building Permits](https://data.detroitmi.gov/Property-Parcels/Building-Permits/xw2a-a7tf)\n",
    "* [Trades Permits](https://data.detroitmi.gov/Property-Parcels/Trades-Permits/635b-dsgv)\n",
    "* [Improve Detroit: Submitted Issues](https://data.detroitmi.gov/Government/Improve-Detroit-Submitted-Issues/fwz3-w3yn)\n",
    "* [DPD: Citizen Complaints](https://data.detroitmi.gov/Public-Safety/DPD-Citizen-Complaints-2016/kahe-efs3)\n",
    "* [Parcel Map](https://data.detroitmi.gov/Property-Parcels/Parcel-Map/fxkw-udwf)\n",
    "\n",
    "___\n",
    "\n",
    "We provide you with two data files for use in training and validating your models: train.csv and test.csv. Each row in these two files corresponds to a single blight ticket, and includes information about when, why, and to whom each ticket was issued. The target variable is compliance, which is True if the ticket was paid early, on time, or within one month of the hearing data, False if the ticket was paid after the hearing date or not at all, and Null if the violator was found not responsible. Compliance, as well as a handful of other variables that will not be available at test-time, are only included in train.csv.\n",
    "\n",
    "Note: All tickets where the violators were found not responsible are not considered during evaluation. They are included in the training set as an additional source of data for visualization, and to enable unsupervised and semi-supervised approaches. However, they are not included in the test set.\n",
    "\n",
    "<br>\n",
    "\n",
    "**File descriptions** (Use only this data for training your model!)\n",
    "\n",
    "    readonly/train.csv - the training set (all tickets issued 2004-2011)\n",
    "    readonly/test.csv - the test set (all tickets issued 2012-2016)\n",
    "    readonly/addresses.csv & readonly/latlons.csv - mapping from ticket id to addresses, and from addresses to lat/lon coordinates. \n",
    "     Note: misspelled addresses may be incorrectly geolocated.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Data fields**\n",
    "\n",
    "train.csv & test.csv\n",
    "\n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "train.csv only\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction] \n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Your predictions will be given as the probability that the corresponding blight ticket will be paid on time.\n",
    "\n",
    "The evaluation metric for this assignment is the Area Under the ROC Curve (AUC). \n",
    "\n",
    "Your grade will be based on the AUC score computed for your classifier. A model which with an AUROC of 0.7 passes this assignment, over 0.75 will recieve full points.\n",
    "___\n",
    "\n",
    "For this assignment, create a function that trains a model to predict blight ticket compliance in Detroit using `readonly/train.csv`. Using this model, return a series of length 61001 with the data being the probability that each corresponding ticket from `readonly/test.csv` will be paid, and the index being the ticket_id.\n",
    "\n",
    "Example:\n",
    "\n",
    "    ticket_id\n",
    "       284932    0.531842\n",
    "       285362    0.401958\n",
    "       285361    0.105928\n",
    "       285338    0.018572\n",
    "                 ...\n",
    "       376499    0.208567\n",
    "       376500    0.818759\n",
    "       369851    0.018528\n",
    "       Name: compliance, dtype: float32\n",
    "       \n",
    "### Hints\n",
    "\n",
    "* Make sure your code is working before submitting it to the autograder.\n",
    "\n",
    "* Print out your result to see whether there is anything weird (e.g., all probabilities are the same).\n",
    "\n",
    "* Generally the total runtime should be less than 10 mins. You should NOT use Neural Network related classifiers (e.g., MLPClassifier) in this question. \n",
    "\n",
    "* Try to avoid global variables. If you have other functions besides blight_model, you should move those functions inside the scope of blight_model.\n",
    "\n",
    "* Refer to the pinned threads in Week 4's discussion forum when there is something you could not figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import cross_validation\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import the dataframes.\n",
    "blight_df_train = pd.read_csv('train.csv', engine='python')\n",
    "blight_df_test = pd.read_csv('test.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159880,)\n",
      "(159880, 27)\n",
      "(61001, 27)\n",
      "\n",
      "0     284932\n",
      "1     285362\n",
      "2     285361\n",
      "3     285338\n",
      "4     285346\n",
      "5     285345\n",
      "6     285347\n",
      "7     285342\n",
      "8     285530\n",
      "9     284989\n",
      "10    285344\n",
      "11    285343\n",
      "12    285340\n",
      "13    285341\n",
      "14    285349\n",
      "Name: ticket_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Avoiding null values in the response label.\n",
    "\n",
    "#Counting the proportion of nan values in the df.\n",
    "y = blight_df_train['compliance']\n",
    "y.columns = ['compliance']\n",
    "\n",
    "#Calculate how many rows in the target variable are nan.\n",
    "nan_percentage = y.isnull().sum()/y.shape[0]\n",
    "\n",
    "#Erease rows having nan values in target.\n",
    "index_nan = y.index[y.apply(np.isnan)].tolist()\n",
    "y = y.drop(y.index[[index_nan]])\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "#Assign to X matrixes the values obtained from the datasets.\n",
    "blight_df_train = blight_df_train.drop(blight_df_train.index[[index_nan]])\n",
    "X = blight_df_train\n",
    "X = X.reset_index(drop=True)\n",
    "features = ['ticket_id','agency_name','inspector_name','violator_name','violation_street_number','violation_street_name','violation_zip_code','mailing_address_str_number','mailing_address_str_name','city','state','zip_code','non_us_str_code','country','ticket_issued_date','hearing_date' ,'violation_code','violation_description','disposition','fine_amount','admin_fee','state_fee','late_fee','discount_amount','clean_up_cost','judgment_amount','grafitti_status']  \n",
    "X = X[features]\n",
    "\n",
    "X_test = blight_df_test\n",
    "X_test = X_test[features]\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "print(X_test.shape)\n",
    "print(\"\")\n",
    "print(X_test['ticket_id'].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Test set adjustment</h3>\n",
    "<p>Guaranteeing the test set meet all the requirements for the evaluation phase.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Define a method that indicates if a column contains NaN values.\n",
    "def column_contains_na(df, df_column_name):\n",
    "    b = df[df_column_name]\n",
    "    contains_nan = b.isnull().values.any()\n",
    "    return contains_nan\n",
    "\n",
    "b = column_contains_na(X_test,'violation_code')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a method that returns the most frequent value of a column given by argument.\n",
    "def column_most_frequent_value(df , df_colum_name):\n",
    "    column = df[df_colum_name]\n",
    "    most_frequent_value = column.mode().values\n",
    "    most_frequent_value = most_frequent_value[0,]\n",
    "    return most_frequent_value\n",
    "\n",
    "#c = column_most_frequent_value(X_test,'violation_zip_code')\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a method that for a column of type date, replace NaN by an ancient date.\n",
    "def column_na_date_replace_by_ancient_date(df, df_colum_name, replacement_value):\n",
    "    \n",
    "    df_updated = df\n",
    "    if(isinstance(df_updated, pd.Series)):\n",
    "        df_updated = pd.DataFrame(df_updated)\n",
    "        df_updated.columns = [df_colum_name]\n",
    "    b = df_updated[df_colum_name]\n",
    "    b_updated = b.fillna(value=replacement_value)\n",
    "    b_updated = pd.DataFrame(b_updated)\n",
    "    df_updated = df_updated.drop(df_colum_name,axis=1)\n",
    "    df_updated = pd.concat([df_updated,b_updated],axis=1)\n",
    "    \n",
    "    return df_updated\n",
    "\n",
    "#b = X_test['hearing_date']\n",
    "#b_updated = column_na_date_replace_by_ancient_date(X_test,'hearing_date','2012-01-19 09:00:00')\n",
    "#contains_na = column_contains_na(b_updated,'hearing_date')\n",
    "#print(\"\")\n",
    "#print(contains_na)\n",
    "#print(b_updated.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a method that for a column of type object, replace NaN by the most frequent column value.\n",
    "def column_na_object_replace_by_frequent(df, df_colum_name):\n",
    "    df_updated = df\n",
    "    replacement_value = column_most_frequent_value(df_updated,df_colum_name)\n",
    "    if(isinstance(df_updated, pd.Series)):\n",
    "        df_updated = pd.DataFrame(df_updated)\n",
    "        df_updated.columns = [df_colum_name]\n",
    "        \n",
    "    b = df_updated[df_colum_name]\n",
    "    b_updated = b.fillna(value=replacement_value)\n",
    "    b_updated = pd.DataFrame(b_updated)\n",
    "    df_updated = df_updated.drop(df_colum_name,axis=1)\n",
    "    df_updated = pd.concat([df_updated,b_updated],axis=1)\n",
    "    \n",
    "    return df_updated\n",
    "\n",
    "#b = column_na_object_replace_by_frequent(X_test, 'violation_zip_code')\n",
    "#c = column_contains_na(b,'violation_zip_code')\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a method that for a column of type object, replace NaN by the most frequent column value.\n",
    "def column_na_float_replace_by_zeros(df, df_colum_name):\n",
    "    df_updated = df\n",
    "    if(isinstance(df_updated, pd.Series)):\n",
    "        df_updated = pd.DataFrame(df_updated)\n",
    "        df_updated.columns = [df_colum_name]\n",
    "        \n",
    "    b = df_updated[df_colum_name]\n",
    "    b_updated = b.fillna(0)\n",
    "    b_updated = pd.DataFrame(b_updated)\n",
    "    df_updated = df_updated.drop(df_colum_name,axis=1)\n",
    "    df_updated = pd.concat([df_updated,b_updated],axis=1)\n",
    "    \n",
    "    return df_updated\n",
    "\n",
    "#b = column_na_float_replace_by_zeros(X_test, 'non_us_str_code')\n",
    "#print(b)\n",
    "#c = column_contains_na(b,'non_us_str_code')\n",
    "#print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a methods that returns true if a column is of type object.\n",
    "def column_is_object(df , df_column_name):\n",
    "    column = df[df_column_name]\n",
    "    if (column.dtype ==\"object\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "#condition = column_is_object(X_test, 'violator_name')\n",
    "#print(condition)\n",
    "#print(X_test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160335\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Define a method that combines all of the adjustments in order to modify the X_test.\n",
    "def adjust_X_test(df):\n",
    "    \n",
    "    df_analysis = df\n",
    "    df_columns = df.columns.tolist()\n",
    "    \n",
    "    for i in df_columns:\n",
    "        \n",
    "        #the column contains nan values?\n",
    "        if(column_contains_na(df_analysis,i)==True):\n",
    "            \n",
    "            #The column is object?\n",
    "            if(column_is_object(df_analysis,i)==True):\n",
    "                #The column contains dates as strings?\n",
    "                if (i=='hearing_date')or(i=='ticket_issued_date'):\n",
    "                    df_analysis = column_na_date_replace_by_ancient_date(df_analysis,i,'2020-01-19 09:00:00')\n",
    "                else:\n",
    "                    df_analysis = column_na_object_replace_by_frequent(df_analysis, i)\n",
    "            else:\n",
    "                df_analysis = column_na_float_replace_by_zeros(df_analysis,i)\n",
    "           \n",
    "    return df_analysis\n",
    "\n",
    "number_of_nulls = X_test.isnull().sum().sum()\n",
    "print(number_of_nulls)\n",
    "\n",
    "X_test = adjust_X_test(X_test)\n",
    "number_of_nulls = X_test.isnull().sum().sum()\n",
    "print(number_of_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61001, 27)\n",
      "0    284932\n",
      "1    285362\n",
      "2    285361\n",
      "3    285338\n",
      "4    285346\n",
      "5    285345\n",
      "6    285347\n",
      "7    285342\n",
      "8    285530\n",
      "9    284989\n",
      "Name: ticket_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_test['ticket_id'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Data preparation</h3>\n",
    "<p>Prepare the datasets X, y, and X_test to resolve the property maintenance fines problem in the city of Detroit, the largest in the state of Michigan, which location in the border between US and Canada.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159880, 27)\n",
      "(159880,)\n",
      "(61001, 27)\n"
     ]
    }
   ],
   "source": [
    "#Delete and update the X in cases where ticket_issued_dates are nan.\n",
    "def delete_rows_ticket_issued_date_is_na(df):\n",
    "    ticket_issued_date = df['ticket_issued_date']\n",
    "    ticket_issued_date = ticket_issued_date.dropna()\n",
    "    index_ticket_issued_date = ticket_issued_date.index.tolist()\n",
    "    ticket_issued_date = ticket_issued_date.reset_index(drop=True)\n",
    "    df_updated = df[df.index.isin(index_ticket_issued_date)]\n",
    "    df_updated = df_updated.reset_index(drop=True)\n",
    "    return df_updated, ticket_issued_date, index_ticket_issued_date\n",
    "\n",
    "X, ticket_issued_date, index_ticket_issued_date = delete_rows_ticket_issued_date_is_na(X)\n",
    "\n",
    "#Update y.\n",
    "y = y[y.index.isin(index_ticket_issued_date)]\n",
    "y = y.reset_index(drop=True)\n",
    "#Update X-test\n",
    "X_test, ticket_issued_date_test, index_ticket_issued_date_test = delete_rows_ticket_issued_date_is_na(X_test)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159653, 27)\n",
      "(159653,)\n",
      "(61001, 27)\n"
     ]
    }
   ],
   "source": [
    "#Delete rows in a df containing NaN values for the column hearing_date.\n",
    "#Returns the updated df, and the hearing df column for further analysis.\n",
    "def delete_rows_hearing_date_is_na(df):\n",
    "    hearing_date = df['hearing_date']\n",
    "    hearing_date = hearing_date.dropna()\n",
    "    index_hearing_date = hearing_date.index.tolist()\n",
    "    hearing_date = hearing_date.reset_index(drop=True)\n",
    "    df_updated = df[df.index.isin(index_hearing_date)]\n",
    "    df_updated = df_updated.reset_index(drop=True)\n",
    "    return df_updated, hearing_date, index_hearing_date\n",
    "\n",
    "X, hearing_date, index_hearing_date = delete_rows_hearing_date_is_na(X)\n",
    "\n",
    "#Update y.\n",
    "y = y[y.index.isin(index_hearing_date)]\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "#Update X-test\n",
    "X_test, hearing_date_test, index_hearing_date_test = delete_rows_hearing_date_is_na(X_test)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159653, 28)\n",
      "(159653,)\n",
      "(61001, 28)\n"
     ]
    }
   ],
   "source": [
    "#Defining a method that calculates the number of months between the ticket issued date and the hearing date features.\n",
    "#Add the column diff as the difference in montsh between colummns ticket issued date and hearing date..\n",
    "#returns the df with the new column.\n",
    "def diff_months_tiket_issued_and_hearing_date(df):\n",
    "    ticket_issued_date_times = []\n",
    "    hearing_date_times = []\n",
    "    n_months_diff = []\n",
    "    \n",
    "    for i in range(0,df.shape[0]):\n",
    "        a = str(df['ticket_issued_date'].values[i])\n",
    "        a = datetime.strptime(a, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "        b = str(df['hearing_date'].values[i])\n",
    "        b = datetime.strptime(b, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "        r = relativedelta.relativedelta(b, a)\n",
    "        n_months = r.months\n",
    "        n_months_diff.append(n_months)\n",
    "        \n",
    "    n_months_diff = pd.DataFrame(n_months_diff)\n",
    "    n_months_diff.columns = ['n_months_diff_issued_hearing']\n",
    "\n",
    "    df_updated = pd.concat([df,n_months_diff], axis =1)\n",
    "    \n",
    "    return(df_updated)\n",
    "\n",
    "#Update X.\n",
    "X = diff_months_tiket_issued_and_hearing_date(X)\n",
    "\n",
    "#Calculate the n_months column for the X_test sample.\n",
    "X_test = diff_months_tiket_issued_and_hearing_date(X_test)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#correct_rows = X_test['n_months_diff_issued_hearing']\n",
    "#correct_rows = pd.DataFrame(correct_rows)\n",
    "#correct_rows.columns = [\"n_months_diff_issued_hearing\"]\n",
    "#n_months_corrections = []\n",
    "\n",
    "#for index, row in correct_rows.iterrows():\n",
    "   # months = row[\"n_months_diff_issued_hearing\"]\n",
    "   # if(months<0):\n",
    "    #    months = 30\n",
    "    #n_months_corrections.append(months)\n",
    "    \n",
    "#n_months_corrections = pd.DataFrame(n_months_corrections)  \n",
    "#n_months_corrections.columns = [\"n_months_diff_issued_hearing\"]\n",
    "    \n",
    "#print(n_months_corrections.head(15))\n",
    "#print(n_months_corrections.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159613, 28)\n",
      "(159613,)\n",
      "(61001, 28)\n"
     ]
    }
   ],
   "source": [
    "#Define a method that deletes rows in which the column hearing contains dates happened later than ticket issued dates.\n",
    "#Deletes rows in which tikets_issued_date is later than hearing_date.\n",
    "#Returns the df modified without unlogic rows.\n",
    "def df_correct_issued_date_later_than_hearing_date(df ,is_test_set=False):\n",
    "    if (is_test_set==False):\n",
    "        correct_rows = df['n_months_diff_issued_hearing']\n",
    "        correct_rows = (correct_rows>=0)\n",
    "        correct_rows = df[correct_rows]\n",
    "        index_correct_rows = correct_rows.index.tolist()\n",
    "        df_updated = df[df.index.isin(index_correct_rows)]\n",
    "        df_updated = df_updated.reset_index(drop=True)\n",
    "        return df_updated, index_correct_rows\n",
    "    else:\n",
    "        correct_rows = df['n_months_diff_issued_hearing']\n",
    "        correct_rows = pd.DataFrame(correct_rows)\n",
    "        correct_rows.columns = [\"n_months_diff_issued_hearing\"]\n",
    "        n_months_corrections = []\n",
    "\n",
    "        for index, row in correct_rows.iterrows():\n",
    "            months = row[\"n_months_diff_issued_hearing\"]\n",
    "            if(months<0):\n",
    "                months = 30\n",
    "            n_months_corrections.append(months)\n",
    "    \n",
    "        n_months_corrections = pd.DataFrame(n_months_corrections)  \n",
    "        n_months_corrections.columns = [\"n_months_diff_issued_hearing\"]\n",
    "        \n",
    "        #Update the dataframe and its column n_months_diff_issued_hearing.\n",
    "        df_updated = df\n",
    "        df_updated = df_updated.drop('n_months_diff_issued_hearing',axis=1)\n",
    "        df_updated = pd.concat([df_updated,n_months_corrections],axis =1)\n",
    "        \n",
    "        return df_updated\n",
    "\n",
    "#Update X.\n",
    "X, index_correct_rows = df_correct_issued_date_later_than_hearing_date(X, False)\n",
    "\n",
    "#Update y.\n",
    "y = y[y.index.isin(index_correct_rows)]\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "#Update X-test\n",
    "X_test = df_correct_issued_date_later_than_hearing_date(X_test, True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159529, 28)\n",
      "(159529,)\n",
      "(61001, 28)\n",
      "\n",
      "(159529,)\n",
      "(61001,)\n",
      "(159529,)\n",
      "(61001,)\n"
     ]
    }
   ],
   "source": [
    "#Delete rows having NaN values in the column state for a dataframe passed by parameter.\n",
    "#Returns the updated df, and the state column for further analysis.\n",
    "def delete_rows_state_is_na(df):\n",
    "    state = df['state']\n",
    "    state = state.dropna()\n",
    "    index_state = state.index.tolist()\n",
    "    state = state.reset_index(drop=True)\n",
    "    df_updated = df[df.index.isin(index_state)]\n",
    "    df_updated = df_updated.reset_index(drop=True)\n",
    "    return df_updated, state, index_state\n",
    "\n",
    "X, state, index_state = delete_rows_state_is_na(X)\n",
    "\n",
    "#Update y\n",
    "y = y[y.index.isin(index_state)]\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "#Update hearing_date\n",
    "hearing_date = hearing_date[hearing_date.index.isin(index_state)]\n",
    "hearing_date = hearing_date.reset_index(drop=True)\n",
    "\n",
    "#X_test\n",
    "X_test, state_test, index_state_test = delete_rows_state_is_na(X_test)\n",
    "\n",
    "#hearing_date_test\n",
    "hearing_date_test = hearing_date_test[hearing_date_test.index.isin(index_state_test)]\n",
    "hearing_date_test = hearing_date_test.reset_index(drop=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(hearing_date.shape)\n",
    "print(hearing_date_test.shape)\n",
    "print(state.shape)\n",
    "print(state_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159529, 28)\n",
      "(159529,)\n",
      "(61001, 28)\n",
      "\n",
      "(159529,)\n",
      "(61001,)\n",
      "(159529,)\n",
      "(61001,)\n",
      "(159529,)\n",
      "(61001,)\n"
     ]
    }
   ],
   "source": [
    "#Delete rows in a df containing NaN values for the column violation_code.\n",
    "#Returns the updated df, and the hearing df column for further analysis.\n",
    "def delete_rows_violation_code_is_na(df):\n",
    "    violation_code = df['violation_code']\n",
    "    violation_code = violation_code.dropna()\n",
    "    index_violation_code = violation_code.index.tolist()\n",
    "    violation_code = violation_code.reset_index(drop=True)\n",
    "    df_updated = df[df.index.isin(index_violation_code)]\n",
    "    df_updated = df_updated.reset_index(drop=True)\n",
    "    return df_updated, violation_code, index_violation_code\n",
    "\n",
    "X, violation_code, index_violation_code = delete_rows_violation_code_is_na(X)\n",
    "\n",
    "#Update y.\n",
    "y = y[y.index.isin(index_violation_code)]\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "#Update X_test.\n",
    "X_test, violation_code_test, index_violation_code_test = delete_rows_violation_code_is_na(X_test)\n",
    "\n",
    "#Update hearing_state\n",
    "hearing_date = hearing_date[hearing_date.index.isin(index_violation_code)]\n",
    "hearing_date = hearing_date.reset_index(drop=True)\n",
    "\n",
    "hearing_date_test = hearing_date_test[hearing_date_test.index.isin(index_violation_code)]\n",
    "hearing_date_test = hearing_date_test.reset_index(drop=True)\n",
    "\n",
    "#Update state.\n",
    "state = state[state.index.isin(index_violation_code)]\n",
    "state = state.reset_index(drop=True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(hearing_date.shape)\n",
    "print(hearing_date_test.shape)\n",
    "print(state.shape)\n",
    "print(state_test.shape)\n",
    "print(violation_code.shape)\n",
    "print(violation_code_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check features containing nan values.\n",
    "def column_contains_nan(column):\n",
    "    contains = False\n",
    "    contains_all = False\n",
    "    nans = column.isnull().sum()\n",
    "    if nans > 0:\n",
    "        contains = True\n",
    "        if nans == column.shape[0]:\n",
    "            contains_all = True\n",
    "    return contains , contains_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id                         int64\n",
      "agency_name                      object\n",
      "inspector_name                   object\n",
      "violator_name                    object\n",
      "violation_street_number         float64\n",
      "violation_street_name            object\n",
      "mailing_address_str_number      float64\n",
      "mailing_address_str_name         object\n",
      "city                             object\n",
      "state                            object\n",
      "zip_code                         object\n",
      "non_us_str_code                  object\n",
      "country                          object\n",
      "ticket_issued_date               object\n",
      "hearing_date                     object\n",
      "violation_code                   object\n",
      "violation_description            object\n",
      "disposition                      object\n",
      "fine_amount                     float64\n",
      "admin_fee                       float64\n",
      "state_fee                       float64\n",
      "late_fee                        float64\n",
      "discount_amount                 float64\n",
      "clean_up_cost                   float64\n",
      "judgment_amount                 float64\n",
      "n_months_diff_issued_hearing      int64\n",
      "dtype: object\n",
      "\n",
      "(159529, 26)\n",
      "(159529,)\n",
      "(61001, 26)\n"
     ]
    }
   ],
   "source": [
    "#Droping variables (columns) in df containing only nan values (all rows are nan).\n",
    "def drop_nan_columns(df):\n",
    "    df_modified = df\n",
    "    for i in df_modified.columns:\n",
    "        contains, contains_all = column_contains_nan(df_modified[i])\n",
    "        if contains_all==True:\n",
    "            df_modified = df_modified.drop([i], axis=1)    \n",
    "    return df_modified\n",
    "\n",
    "#Update X.\n",
    "X= drop_nan_columns(X)\n",
    "\n",
    "#Update X_test.\n",
    "features = X.columns\n",
    "X_test = X_test[features]\n",
    "\n",
    "print(X.dtypes)\n",
    "\n",
    "print(\"\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id                         int64\n",
      "agency_name                      object\n",
      "inspector_name                   object\n",
      "violator_name                    object\n",
      "violation_street_name            object\n",
      "mailing_address_str_name         object\n",
      "city                             object\n",
      "state                            object\n",
      "zip_code                         object\n",
      "country                          object\n",
      "ticket_issued_date               object\n",
      "hearing_date                     object\n",
      "violation_code                   object\n",
      "violation_description            object\n",
      "disposition                      object\n",
      "fine_amount                     float64\n",
      "admin_fee                       float64\n",
      "state_fee                       float64\n",
      "late_fee                        float64\n",
      "discount_amount                 float64\n",
      "clean_up_cost                   float64\n",
      "judgment_amount                 float64\n",
      "n_months_diff_issued_hearing      int64\n",
      "dtype: object\n",
      "\n",
      "(159529, 23)\n",
      "(159529,)\n",
      "(61001, 23)\n"
     ]
    }
   ],
   "source": [
    "#Define a method that drops from the set of features the variables that aren't relevant in the analysis. \n",
    "#A list with the name of irrelevant variables is passed by argument.\n",
    "def drop_not_relevant_column(df,list_columns):\n",
    "    df_modified = df\n",
    "    for i in list_columns:\n",
    "        df_modified = df_modified.drop([i], axis=1)   \n",
    "    return df_modified\n",
    "\n",
    "columns_to_delete = ['violation_street_number','non_us_str_code','mailing_address_str_number']\n",
    "\n",
    "#Update X.\n",
    "X = drop_not_relevant_column(X,columns_to_delete)\n",
    "\n",
    "#Update X_test.\n",
    "features = X.columns\n",
    "X_test = X_test[features]\n",
    "\n",
    "print(X.dtypes)\n",
    "print(\"\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "                   ratio_best_payed\n",
      "inspector_name                     \n",
      "Thomas, Duane                   1.0\n",
      "Coleman, Lanetha                1.0\n",
      "Hischke, William                1.0\n",
      "BENNETT, MARGARET               0.5\n",
      "Long, Phil                      0.4\n",
      "['Thomas, Duane', 'Coleman, Lanetha', 'Hischke, William', 'BENNETT, MARGARET', 'Long, Phil', 'Pickens, William', 'Frinkley, Elaine', 'Madrigal, Michael', 'Wilcox, Valerie', 'Gibson, Christopher', 'Brinkley, Kevin', 'Crowder, Michael', 'Johnson, Clifford', 'Smith, Melvin', 'Anderson, Trevis', 'Shah , Kumarpal', 'Copty, Anton', 'Zawislak, Norbet', 'Holbrook, Kevin', 'Fulks, Matthew g']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Define a method that creates a contingency table with the values in a column that maximixes the compliance rate..\n",
    "#A column name should be passed by argument.\n",
    "#The contingency contains the n_top values with more payed tickets in the columns passed by parameter..\n",
    "#Gives the columns values having the best compliance ratio.\n",
    "#Returns a df with the ordered values, and a list with their name.\n",
    "def find_best_ratio_of_payment_column(df_column, df_target, df_column_name, df_target_name, n_top):\n",
    "  \n",
    "    df = pd.concat([df_column, df_target], axis =1)\n",
    "    \n",
    "    #Create contingency table df.\n",
    "    df_contingency = pd.crosstab(df[df_column_name], df[df_target_name])\n",
    "    contingency = df_contingency.values\n",
    "    \n",
    "    #Calculate the ratio of best payed tickets regarding on the feature values.\n",
    "    ratio_best_payed = contingency[:,1]/np.sum(contingency,axis=1)\n",
    "    ratio_best_payed = pd.DataFrame(ratio_best_payed)\n",
    "    ratio_best_payed.index = df_contingency.index\n",
    "    ratio_best_payed.columns = ['ratio_best_payed']\n",
    "    ratio_best_payed = ratio_best_payed.sort_values(by=['ratio_best_payed'], ascending=False)\n",
    "    \n",
    "    #Select the n_top of the contingency ratio df.\n",
    "    ratio_best_payed = ratio_best_payed.iloc[0:n_top,:]\n",
    "    list_names = ratio_best_payed.index.tolist()\n",
    "    \n",
    "    #Return df_ratio from the contigency with the ordered values.\n",
    "    return(ratio_best_payed ,list_names)\n",
    "    \n",
    "ratio_best_payed_inspector_name, list_names_ratio_best_inspector_name = find_best_ratio_of_payment_column(X['inspector_name'], y, \"inspector_name\", \"compliance\", 20)\n",
    "\n",
    "print(ratio_best_payed_inspector_name.shape)\n",
    "print(ratio_best_payed_inspector_name.head(5))\n",
    "\n",
    "print(list_names_ratio_best_inspector_name)\n",
    "print(len(list_names_ratio_best_inspector_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  n_tickets\n",
      "inspector_name             \n",
      "Morris, John          11604\n",
      "Samaan, Neil J         8699\n",
      "O'Neal, Claude         8067\n",
      "Steele, Jonathan       6957\n",
      "Devaney, John          6837\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Define a method that creates a df having the contingency values for the n_top most frequent values.\n",
    "#Returns a df with the n_top values having more ticktes\n",
    "def most_frequent_values_in_feature(df_column, df_y,  n_top):\n",
    "    \n",
    "    df_contingency_1 = pd.crosstab(df_column, y)\n",
    "    df_contingency = df_contingency_1.values\n",
    "    n_tickets = np.sum(df_contingency,axis=1)\n",
    "    n_tickets = pd.DataFrame(n_tickets)\n",
    "    n_tickets.index = df_contingency_1.index\n",
    "    n_tickets.columns = ['n_tickets']\n",
    "    n_tickets = n_tickets.sort_values(by=['n_tickets'], ascending=False)\n",
    "    n_tickets = n_tickets.iloc[0:n_top,:]\n",
    "    list_names = n_tickets.index.tolist()\n",
    "    \n",
    "    #Return df_ratio from the contigency with the ordered values.\n",
    "    return(n_tickets ,list_names)\n",
    "\n",
    "df_frequents_inspector_name, list_names_inspector_name = most_frequent_values_in_feature(X['inspector_name'], y , 5)\n",
    "\n",
    "print(df_frequents_inspector_name)\n",
    "print(len(list_names_inspector_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159529, 27)\n",
      "(159529,)\n",
      "(61001, 27)\n"
     ]
    }
   ],
   "source": [
    "#Define a method that encodes dataframe categorical variables.\n",
    "#A list of columns to encode, and the name of the column must be passed by argument.\n",
    "#Returns the updated df with the encodings.\n",
    "def encode_dummy_given_list(df, list_names, column_name):    \n",
    "    df_update = df\n",
    "    for i in list_names:\n",
    "        column = (df_update[column_name]==i).astype(int)\n",
    "        column = pd.DataFrame(column)\n",
    "        column.columns = [column_name+\"_\"+i]\n",
    "        df_update = pd.concat([df_update,column], axis =1)\n",
    "        \n",
    "    df_update = df_update.drop(column_name, axis=1)\n",
    "    \n",
    "    return df_update\n",
    "\n",
    "X = encode_dummy_given_list(X,list_names_inspector_name,\"inspector_name\")\n",
    "\n",
    "#Update X_test\n",
    "X_test = encode_dummy_given_list(X_test,list_names_inspector_name,\"inspector_name\")\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "                      ratio_best_payed\n",
      "violation_code                        \n",
      "61-47.0000/47.0108                 1.0\n",
      "9-1-83 - (Building 5               0.5\n",
      "19840901                           0.5\n",
      "22-2-87(a)                         0.5\n",
      "9-1-333                            0.5\n",
      "\n",
      "['61-47.0000/47.0108', '9-1-83 - (Building 5', '19840901', '22-2-87(a)', '9-1-333']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Calculate best ratio payments for violation code.\n",
    "ratio_best_payed_violation_code, list_names_ratio_best_violation_code = find_best_ratio_of_payment_column(violation_code, y, \"violation_code\", \"compliance\", 5)\n",
    "\n",
    "print(ratio_best_payed_violation_code.shape)\n",
    "print(ratio_best_payed_violation_code.head(5))\n",
    "print(\"\")\n",
    "print(list_names_ratio_best_violation_code)\n",
    "print(len(list_names_ratio_best_violation_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                n_tickets\n",
      "violation_code           \n",
      "9-1-36(a)           64335\n",
      "9-1-81(a)           23133\n",
      "22-2-88             18989\n",
      "9-1-104             16871\n",
      "22-2-88(b)           4821\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Calculate the most frequent values for the column violation_code.\n",
    "df_frequents_violation_code, list_names_violation_code = most_frequent_values_in_feature(violation_code, y , 5)\n",
    "\n",
    "print(df_frequents_violation_code)\n",
    "print(len(list_names_violation_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159529, 31)\n",
      "(159529,)\n",
      "(61001, 31)\n"
     ]
    }
   ],
   "source": [
    "#Encode values of violation_code found with the best violation_code..\n",
    "#Update X.\n",
    "X = encode_dummy_given_list(X, list_names_violation_code, \"violation_code\")\n",
    "\n",
    "#Update X_test.\n",
    "X_test = encode_dummy_given_list(X_test, list_names_violation_code, \"violation_code\")\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "       ratio_best_payed\n",
      "state                  \n",
      "QL             1.000000\n",
      "HI             0.333333\n",
      "RI             0.266667\n",
      "KS             0.230769\n",
      "NH             0.222222\n",
      "\n",
      "['QL', 'HI', 'RI', 'KS', 'NH', 'NB', 'KY', 'CO', 'SD', 'OH', 'IL', 'MD', 'AK', 'WY', 'WA', 'NY', 'MI', 'NJ', 'WI', 'ON']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Calculate best ratio payments for the column state.\n",
    "ratio_best_payed_state, list_names_ratio_best_state = find_best_ratio_of_payment_column(state, y, \"state\", \"compliance\", 20)\n",
    "\n",
    "print(ratio_best_payed_state.shape)\n",
    "print(ratio_best_payed_state.head(5))\n",
    "print(\"\")\n",
    "print(list_names_ratio_best_state)\n",
    "print(len(list_names_ratio_best_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       n_tickets\n",
      "state           \n",
      "MI        143415\n",
      "CA          3823\n",
      "TX          1946\n",
      "FL          1679\n",
      "SC          1066\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Calculate the most frequent values for the column state.\n",
    "df_frequents_state, list_names_state = most_frequent_values_in_feature(state, y , 5)\n",
    "\n",
    "print(df_frequents_state)\n",
    "print(len(list_names_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159529, 35)\n",
      "(159529,)\n",
      "(61001, 35)\n"
     ]
    }
   ],
   "source": [
    "#Encode values of violation_code found with the best state ratio.\n",
    "#Update X.\n",
    "X = encode_dummy_given_list(X, list_names_state, \"state\")\n",
    "\n",
    "#Update X_test.\n",
    "X_test = encode_dummy_given_list(X_test, list_names_state, \"state\")\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id                            int64\n",
      "agency_name                         object\n",
      "country                             object\n",
      "disposition                         object\n",
      "fine_amount                        float64\n",
      "admin_fee                          float64\n",
      "state_fee                          float64\n",
      "late_fee                           float64\n",
      "discount_amount                    float64\n",
      "clean_up_cost                      float64\n",
      "judgment_amount                    float64\n",
      "n_months_diff_issued_hearing         int64\n",
      "inspector_name_Morris, John          int64\n",
      "inspector_name_Samaan, Neil J        int64\n",
      "inspector_name_O'Neal, Claude        int64\n",
      "inspector_name_Steele, Jonathan      int64\n",
      "inspector_name_Devaney, John         int64\n",
      "violation_code_9-1-36(a)             int64\n",
      "violation_code_9-1-81(a)             int64\n",
      "violation_code_22-2-88               int64\n",
      "violation_code_9-1-104               int64\n",
      "violation_code_22-2-88(b)            int64\n",
      "state_MI                             int64\n",
      "state_CA                             int64\n",
      "state_TX                             int64\n",
      "state_FL                             int64\n",
      "state_SC                             int64\n",
      "dtype: object\n",
      "\n",
      "(159529, 27)\n",
      "(159529,)\n",
      "(61001, 27)\n"
     ]
    }
   ],
   "source": [
    "#Droping variables (columns) having too many categories.\n",
    "def drop_too_many_categories_in_column(df):\n",
    "    df_modified = df\n",
    "    for i in df_modified.columns:\n",
    "        if(i != 'ticket_id'):\n",
    "            if(df_modified.dtypes[i]=='object'):\n",
    "                n_classes = df_modified[i].nunique()\n",
    "                if n_classes >10:\n",
    "                    df_modified = df_modified.drop([i], axis=1)   \n",
    "    return df_modified\n",
    "\n",
    "#Update X.\n",
    "X = drop_too_many_categories_in_column(X)\n",
    "\n",
    "#Update X_test.\n",
    "features = X.columns\n",
    "X_test = X_test[features]\n",
    "\n",
    "print(X.dtypes)\n",
    "\n",
    "print(\"\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(159529, 27)\n",
      "(159529,)\n",
      "(61001, 27)\n",
      "\n",
      "(159529, 1)\n",
      "(159529, 1)\n",
      "\n",
      "(61001, 1)\n",
      "(61001, 1)\n"
     ]
    }
   ],
   "source": [
    "#Extract from the hearing date column the status and month for further analysis.\n",
    "#Two additional features are created.\n",
    "def hearing_date_feature_extraction(df_hearing_date):\n",
    "    \n",
    "    #tranform the type of the column hearing_date into date. \n",
    "    hearing_date = pd.to_datetime(df_hearing_date)\n",
    "    \n",
    "    #Extract the years column from hearing_date.\n",
    "    hearing_date_years = hearing_date.dt.year\n",
    "    hearing_date_years = pd.DataFrame(hearing_date_years)\n",
    "    hearing_date_years.rename(columns = {'hearing_date':'hearing_date_year'}, inplace = True)\n",
    "    hearing_date_years = hearing_date_years.astype('float64')\n",
    "    \n",
    "    #Extract the months column from hearing_date.\n",
    "    hearing_date_months = hearing_date.dt.month\n",
    "    hearing_date_months = pd.DataFrame(hearing_date_months)\n",
    "    hearing_date_months.rename(columns = {'hearing_date':'hearing_date_month'}, inplace = True)\n",
    "    hearing_date_months = hearing_date_months.astype('float64')\n",
    "    \n",
    "    #Extract the column hearing_date_status from the df.\n",
    "    conditions = [\n",
    "        (hearing_date_years['hearing_date_year'] <= 2011),\n",
    "        (hearing_date_years['hearing_date_year'] > 2011) & (hearing_date_years['hearing_date_year'] <= 2015),\n",
    "        (hearing_date_years['hearing_date_year'] > 2015)\n",
    "    ]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    values = ['old', 'partially-old', 'recent']\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    hearing_date_status = pd.DataFrame(np.select(conditions, values))\n",
    "    hearing_date_status.rename(columns = {0: 'hearing_date_status'}, inplace = True)\n",
    "    \n",
    "    #Reset the indexes of the created dataframes.\n",
    "    hearing_date_status = hearing_date_status.reset_index(drop=True)\n",
    "    hearing_date_months = hearing_date_months.reset_index(drop=True)\n",
    "    \n",
    "    #Change the types of the hearing_date_months column.\n",
    "    hearing_date_months = pd.DataFrame(hearing_date_months)\n",
    "    hearing_date_months = hearing_date_months.astype(str)\n",
    "    \n",
    "    #return the extracted columns.\n",
    "    return hearing_date_status, hearing_date_months\n",
    "\n",
    "#Extract the new variables from the feauture X set.\n",
    "hearing_date_status, hearing_date_months = hearing_date_feature_extraction(hearing_date)\n",
    "\n",
    "#Extract the new variables from the feauture X_test set.\n",
    "hearing_date_status_test, hearing_date_months_test = hearing_date_feature_extraction(hearing_date_test)\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"\")\n",
    "print(hearing_date_status.shape)\n",
    "print(hearing_date_months.shape)\n",
    "print(\"\")\n",
    "print(hearing_date_status_test.shape)\n",
    "print(hearing_date_months_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Engineering Data Analysis (EDA)</h3>\n",
    "<p>Check the relevance of some categorical features in relationship with the target variable.The analysis focuses over some extracted variables during the preparation phase.</p>\n",
    "<ul>\n",
    "<li>The variables of interest are n_months_difference, hearing_status, state, inspector_name, and violation_code.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check the changes in the target variable in regards to the variable n_months_difference. \n",
    "#df_diff_months = pd.concat([X['n_months_diff_issued_hearing'], y], axis =1)\n",
    "\n",
    "#sns.countplot(data=df_diff_months, x='n_months_diff_issued_hearing',hue = 'compliance')\n",
    "#plt.show()\n",
    "\n",
    "#print(\"contingency_table\")\n",
    "#print(pd.crosstab(df_diff_months[\"n_months_diff_issued_hearing\"], df_diff_months[\"compliance\"]))\n",
    "\n",
    "#print(\"\")\n",
    "#print(X['n_months_diff_issued_hearing'].unique())\n",
    "#print(X.groupby('n_months_diff_issued_hearing').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Analyse whether the column hearing_date_status is relevant for the target.\n",
    "#Countplots are created.\n",
    "#df_status = pd.concat([y, hearing_date_status], axis =1)\n",
    "#print(df_status.shape)\n",
    "#sns.countplot(data=df_status, x='hearing_date_status',hue = 'compliance')\n",
    "#plt.show()\n",
    "\n",
    "#print(\"contingency_table\")\n",
    "#print(pd.crosstab(df_status[\"hearing_date_status\"], df_status[\"compliance\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Analyse whether the column violation_code is relevant for the target.\n",
    "#Countplots are created.\n",
    "#df_violation_code = pd.concat([violation_code, y], axis =1)\n",
    "#print(df_violation_code.shape)\n",
    "#sns.countplot(data=df_violation_code, x='violation_code',hue = 'compliance')\n",
    "#plt.show()\n",
    "#print(df_violation_code.shape)\n",
    "#print(\"\")\n",
    "#print(\"contingency_table\")\n",
    "#print(pd.crosstab(df_violation_code[\"violation_code\"], df_violation_code[\"compliance\"]))\n",
    "#The most relevant are 19450901, 22-2-17, 22-2-43, 22-2-45, 9-1-81(a).s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Analyse whether the column hearing_dates_months is relevant for the target.\n",
    "#Countplots are created.\n",
    "#df_month = pd.concat([hearing_date_months,y], axis =1)\n",
    "#print(df_month.shape)\n",
    "#sns.countplot(data=df_month, x='hearing_date_month',hue = 'compliance')\n",
    "#plt.show()\n",
    "\n",
    "#print(\"contingency_table\")\n",
    "#print(pd.crosstab(df_month[\"hearing_date_month\"], df_month[\"compliance\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    0.928421\n",
      "1.0    0.071579\n",
      "Name: values, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Check the balance of classes in the target variable.\n",
    "#The relationship is almost 1 to 9 for the classes in the target variable.\n",
    "y_classes = pd.DataFrame(y)\n",
    "y_classes.columns = ['values']\n",
    "print(y_classes['values'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Dataset encoding</h3>\n",
    "<p>Transforming the dateset into all numerical entries. Features containing categorical values are converted into dummy vectors. It means that each category is represented by a vector of ones (1) and zeros (0).</p>\n",
    "<ul>\n",
    "<li>The procedure is done over the whole dataset because features on the train, and test sets might be equal.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159529,)\n",
      "ticket_id                                                       int64\n",
      "fine_amount                                                   float64\n",
      "admin_fee                                                     float64\n",
      "state_fee                                                     float64\n",
      "late_fee                                                      float64\n",
      "discount_amount                                               float64\n",
      "clean_up_cost                                                 float64\n",
      "judgment_amount                                               float64\n",
      "n_months_diff_issued_hearing                                    int64\n",
      "inspector_name_Morris, John                                     int64\n",
      "inspector_name_Samaan, Neil J                                   int64\n",
      "inspector_name_O'Neal, Claude                                   int64\n",
      "inspector_name_Steele, Jonathan                                 int64\n",
      "inspector_name_Devaney, John                                    int64\n",
      "violation_code_9-1-36(a)                                        int64\n",
      "violation_code_9-1-81(a)                                        int64\n",
      "violation_code_22-2-88                                          int64\n",
      "violation_code_9-1-104                                          int64\n",
      "violation_code_22-2-88(b)                                       int64\n",
      "state_MI                                                        int64\n",
      "state_CA                                                        int64\n",
      "state_TX                                                        int64\n",
      "state_FL                                                        int64\n",
      "state_SC                                                        int64\n",
      "agency_name_Buildings, Safety Engineering & Env Department      uint8\n",
      "agency_name_Department of Public Works                          uint8\n",
      "agency_name_Detroit Police Department                           uint8\n",
      "agency_name_Health Department                                   uint8\n",
      "agency_name_Neighborhood City Halls                             uint8\n",
      "country_Aust                                                    uint8\n",
      "country_Cana                                                    uint8\n",
      "country_Germ                                                    uint8\n",
      "country_USA                                                     uint8\n",
      "disposition_Responsible (Fine Waived) by Admis                  uint8\n",
      "disposition_Responsible (Fine Waived) by Deter                  uint8\n",
      "disposition_Responsible - Compl/Adj by Default                  uint8\n",
      "disposition_Responsible - Compl/Adj by Determi                  uint8\n",
      "disposition_Responsible by Admission                            uint8\n",
      "disposition_Responsible by Default                              uint8\n",
      "disposition_Responsible by Determination                        uint8\n",
      "disposition_Responsible by Dismissal                            uint8\n",
      "dtype: object\n",
      "\n",
      "(159529, 41)\n",
      "(159529,)\n",
      "(61001, 41)\n"
     ]
    }
   ],
   "source": [
    "#Define the number of rows in the X matrix.\n",
    "n_X = X.shape[0]\n",
    "\n",
    "#Concatenate vertically X, and X_test matrixes and encode them together.\n",
    "concatenated_dataframes = pd.concat([X, X_test], axis=0)\n",
    "concatenated_dataframes = pd.get_dummies(concatenated_dataframes)\n",
    "\n",
    "#Split the X and X_test matrixes after been encoded.\n",
    "X = concatenated_dataframes.iloc[0:n_X,:]\n",
    "X_test = concatenated_dataframes.iloc[n_X:,:]\n",
    "\n",
    "#Reshaping the y matrix into a vector (n,).\n",
    "y = y.values.reshape(y.values.shape[0],)\n",
    "print(y.shape)\n",
    "\n",
    "print(X.dtypes)\n",
    "print(\"\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4. Feature selection (dimensionality reduction)</h3>\n",
    "<p>Select features having the strongest relationship with the target variable. Dimensionality reduction guarantees a best performance during the training phase</p>\n",
    "<ul>\n",
    "<li>In order to select the features, information gain is applied. It implies that features are selected satisfying the smallest lost of explained information when removing the variables</li>\n",
    "<li>Information gain is done by minimizing the entropy of the matrix X once variables are put taken off/li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      values\n",
      "disposition_Responsible by Default                  0.064177\n",
      "judgment_amount                                     0.052145\n",
      "late_fee                                            0.048161\n",
      "state_fee                                           0.036437\n",
      "admin_fee                                           0.036093\n",
      "country_USA                                         0.035244\n",
      "state_MI                                            0.026306\n",
      "ticket_id                                           0.023753\n",
      "disposition_Responsible by Admission                0.018638\n",
      "discount_amount                                     0.015319\n",
      "agency_name_Buildings, Safety Engineering & Env...  0.012201\n",
      "disposition_Responsible by Determination            0.012159\n",
      "fine_amount                                         0.011938\n",
      "violation_code_9-1-36(a)                            0.005580\n",
      "n_months_diff_issued_hearing                        0.004862\n",
      "['disposition_Responsible by Default', 'judgment_amount', 'late_fee', 'state_fee', 'admin_fee', 'country_USA', 'state_MI', 'ticket_id', 'disposition_Responsible by Admission', 'discount_amount', 'agency_name_Buildings, Safety Engineering & Env Department', 'disposition_Responsible by Determination', 'fine_amount', 'violation_code_9-1-36(a)', 'n_months_diff_issued_hearing']\n",
      "(159529, 15)\n",
      "(61001, 15)\n"
     ]
    }
   ],
   "source": [
    "#Define a method that performs feature selection on the set of features.\n",
    "#Select the n_top feauture variables from the dataset X which better explain y regarding on information gain.\n",
    "#Information gain is based on entropy minimization when selection features.\n",
    "#Returns a list with the names of the selected n_features.\n",
    "def feature_selection_information_gain(X, y, n_top_features):\n",
    "    mutual_info = mutual_info_classif(X,y)\n",
    "    mutual_info = pd.DataFrame(mutual_info)\n",
    "    mutual_info.index = X.columns\n",
    "    mutual_info.columns = ['values']\n",
    "    mutual_info.sort_values(by='values', ascending=False, inplace =True)\n",
    "    \n",
    "    df_relevant_features = mutual_info\n",
    "    df_relevant_features = df_relevant_features.iloc[0:n_top_features,:]\n",
    "    selected_features = df_relevant_features.index.tolist()\n",
    "    print(df_relevant_features)\n",
    "    return(selected_features)\n",
    "\n",
    "selected_features = feature_selection_information_gain(X, y, 15)\n",
    "\n",
    "#Select the features according to their importance\n",
    "X = X[selected_features]\n",
    "X_test = X_test[selected_features]\n",
    "\n",
    "print(selected_features)\n",
    "print(X.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disposition_Responsible by Default', 'judgment_amount', 'late_fee', 'state_fee', 'admin_fee', 'country_USA', 'state_MI', 'ticket_id', 'disposition_Responsible by Admission', 'discount_amount', 'agency_name_Buildings, Safety Engineering & Env Department', 'disposition_Responsible by Determination', 'fine_amount', 'violation_code_9-1-36(a)', 'n_months_diff_issued_hearing']\n"
     ]
    }
   ],
   "source": [
    "#Select the best K features.\n",
    "def feature_selection_information_gain_KBest(X, y, n_top):\n",
    "    sel_cols = SelectKBest(mutual_info_classif, k = n_top)\n",
    "    sel_cols.fit(X,y)\n",
    "    selected_features = X.columns[sel_cols.get_support()].tolist()\n",
    "    return(selected_features)\n",
    "\n",
    "selected_features_KBest = feature_selection_information_gain_KBest(X, y, 15)\n",
    "\n",
    "print(selected_features_KBest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>5. Training phase and model selection</h3>\n",
    "<p>Training multiple classifiers on the X matrix. After models are trained, the one having the best performance is selected.</p>\n",
    "<ul>\n",
    "<li>Hyperparameter tuning is done over each model. No more than 30 mins of training per model is satisfied</li>\n",
    "<li>Classifiers like SVM, Logistic regression, Random forest, and Gradient boosting are used for the purpose of this phase.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a method that returns a series object with the ids and probabilities of compliance.\n",
    "def get_probs_format(X, y_prob):\n",
    "    ticket_predictions = pd.DataFrame(y_prob)\n",
    "    ticket_predictions.index = X[\"ticket_id\"]\n",
    "    ticket_predictions = ticket_predictions.iloc[:,1:2]\n",
    "    ticket_predictions.columns = [\"compliance\"]\n",
    "    ticket_predictions = ticket_predictions.squeeze()\n",
    "    return ticket_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#n_months_diff_issued_hearing\n",
    "print(X_test['n_months_diff_issued_hearing'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split0_test_score': array([ 0.7296437 ,  0.7300794 ,  0.75147205,  0.73593671,  0.75538164,\n",
      "        0.74791049,  0.75655579,  0.75491986,  0.75668577,  0.75624926]), 'split1_test_score': array([ 0.72286973,  0.74555479,  0.77688246,  0.7587999 ,  0.78194468,\n",
      "        0.7753049 ,  0.78211259,  0.78120291,  0.78213495,  0.78200429]), 'split2_test_score': array([ 0.78209667,  0.78901652,  0.82300475,  0.80588357,  0.83214228,\n",
      "        0.82134168,  0.8330905 ,  0.83049403,  0.83317444,  0.83283282]), 'split3_test_score': array([ 0.73439603,  0.73414522,  0.76750971,  0.74542214,  0.76893614,\n",
      "        0.76139743,  0.76912173,  0.77012506,  0.76872657,  0.76907289]), 'split4_test_score': array([ 0.75844167,  0.74597382,  0.76116849,  0.75503033,  0.76206051,\n",
      "        0.76213718,  0.76112213,  0.76576007,  0.76095419,  0.76556319]), 'mean_test_score': array([ 0.74548948,  0.74895397,  0.77600758,  0.76021456,  0.78009316,\n",
      "        0.77361841,  0.78040067,  0.78050048,  0.78033531,  0.78114459]), 'std_test_score': array([ 0.02187995,  0.02098097,  0.02491553,  0.02417761,  0.0274695 ,\n",
      "        0.02538676,  0.02773668,  0.02637863,  0.02780258,  0.02713194]), 'rank_test_score': array([10,  9,  6,  8,  5,  7,  3,  2,  4,  1], dtype=int32), 'split0_train_score': array([ 0.77170359,  0.76845438,  0.78833318,  0.77468754,  0.79304472,\n",
      "        0.78627668,  0.79333943,  0.79192806,  0.79341438,  0.79322192]), 'split1_train_score': array([ 0.76506028,  0.77445311,  0.79140523,  0.78273434,  0.79421681,\n",
      "        0.7913806 ,  0.79469404,  0.79390017,  0.79473688,  0.7944978 ]), 'split2_train_score': array([ 0.75489896,  0.7597938 ,  0.77739542,  0.76740491,  0.78298325,\n",
      "        0.77740388,  0.78336505,  0.78254636,  0.78333447,  0.78328924]), 'split3_train_score': array([ 0.77087675,  0.7678779 ,  0.79584512,  0.77674922,  0.79999332,\n",
      "        0.79253644,  0.80043471,  0.79915991,  0.80049903,  0.8003091 ]), 'split4_train_score': array([ 0.78005739,  0.7694548 ,  0.79733537,  0.77600782,  0.79974745,\n",
      "        0.79204301,  0.79997062,  0.79958627,  0.80001868,  0.79996201]), 'mean_train_score': array([ 0.76851939,  0.7680068 ,  0.79006286,  0.77551677,  0.79399711,\n",
      "        0.78792812,  0.79436077,  0.79342415,  0.79440069,  0.79425601]), 'std_train_score': array([ 0.0083223 ,  0.00471851,  0.0070934 ,  0.00490598,  0.00618516,\n",
      "        0.00571971,  0.00617128,  0.00619131,  0.00620159,  0.00617408]), 'mean_fit_time': array([  0.95948772,   0.74407325,   6.13543615,   1.00599203,\n",
      "        19.34635339,   1.77035189,  37.49232998,   2.06475196,\n",
      "        41.86008511,   2.13406544]), 'std_fit_time': array([  1.36196093e-01,   4.54701704e-02,   2.01189218e+00,\n",
      "         9.32248126e-03,   4.61939532e+00,   1.34768418e-01,\n",
      "         1.37646577e+01,   1.24917143e-01,   1.95989171e+01,\n",
      "         9.11738655e-02]), 'mean_score_time': array([ 0.128719  ,  0.10880241,  0.17858944,  0.18221064,  0.08807783,\n",
      "        0.15958033,  0.1340868 ,  0.18578124,  0.17269359,  0.19678674]), 'std_score_time': array([ 0.10870783,  0.08127811,  0.01251176,  0.00951495,  0.04783244,\n",
      "        0.08225974,  0.05085002,  0.0359861 ,  0.06391074,  0.03312981]), 'param_model__C': masked_array(data = [0.01 0.01 0.1 0.1 1 1 10 10 100 100],\n",
      "             mask = [False False False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_model__penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2'],\n",
      "             mask = [False False False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'params': ({'model__C': 0.01, 'model__penalty': 'l1'}, {'model__C': 0.01, 'model__penalty': 'l2'}, {'model__C': 0.1, 'model__penalty': 'l1'}, {'model__C': 0.1, 'model__penalty': 'l2'}, {'model__C': 1, 'model__penalty': 'l1'}, {'model__C': 1, 'model__penalty': 'l2'}, {'model__C': 10, 'model__penalty': 'l1'}, {'model__C': 10, 'model__penalty': 'l2'}, {'model__C': 100, 'model__penalty': 'l1'}, {'model__C': 100, 'model__penalty': 'l2'})}\n",
      "\n",
      "train\n",
      "[[ 0.76851939  0.7680068 ]\n",
      " [ 0.79006286  0.77551677]\n",
      " [ 0.79399711  0.78792812]\n",
      " [ 0.79436077  0.79342415]\n",
      " [ 0.79440069  0.79425601]]\n",
      "\n",
      "test\n",
      "[[ 0.74548948  0.74895397]\n",
      " [ 0.77600758  0.76021456]\n",
      " [ 0.78009316  0.77361841]\n",
      " [ 0.78040067  0.78050048]\n",
      " [ 0.78033531  0.78114459]]\n",
      "\n",
      "{'model__C': 100, 'model__penalty': 'l2'}\n",
      "\n",
      "[[ 0.9637608   0.0362392 ]\n",
      " [ 0.89385086  0.10614914]\n",
      " [ 0.97783263  0.02216737]\n",
      " ..., \n",
      " [ 0.9601005   0.0398995 ]\n",
      " [ 0.51891431  0.48108569]\n",
      " [ 0.94234777  0.05765223]]\n",
      "ticket_id\n",
      "22056     0.036239\n",
      "27586     0.106149\n",
      "22046     0.022167\n",
      "18738     0.007695\n",
      "18735     0.017635\n",
      "18733     0.017635\n",
      "28204     0.007016\n",
      "18743     0.008508\n",
      "18741     0.008508\n",
      "18978     0.008509\n",
      "18746     0.182239\n",
      "18744     0.182239\n",
      "26846     0.008566\n",
      "26848     0.008566\n",
      "28209     0.008156\n",
      "19950     0.220607\n",
      "18645     0.039234\n",
      "18651     0.039234\n",
      "18649     0.039234\n",
      "18664     0.035593\n",
      "18646     0.037371\n",
      "18661     0.037371\n",
      "18657     0.282443\n",
      "18652     0.039234\n",
      "18665     0.037371\n",
      "18650     0.039234\n",
      "18653     0.978426\n",
      "18658     0.234911\n",
      "18666     0.037371\n",
      "18655     0.917787\n",
      "            ...   \n",
      "285113    0.020293\n",
      "286250    0.166563\n",
      "286251    0.382191\n",
      "285114    0.020293\n",
      "285115    0.065978\n",
      "285116    0.065978\n",
      "284870    0.021320\n",
      "284873    0.021320\n",
      "284871    0.021320\n",
      "284875    0.065110\n",
      "284874    0.065110\n",
      "285091    0.069164\n",
      "285508    0.997357\n",
      "285093    0.069164\n",
      "285095    0.069164\n",
      "285121    0.065978\n",
      "285122    0.065978\n",
      "285120    0.065978\n",
      "285123    0.065978\n",
      "285092    0.069164\n",
      "285094    0.069164\n",
      "285096    0.021324\n",
      "285036    0.069161\n",
      "285037    0.065118\n",
      "285034    0.039897\n",
      "285106    0.054998\n",
      "284650    0.021316\n",
      "285125    0.039899\n",
      "284881    0.481086\n",
      "284333    0.057652\n",
      "Name: compliance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Train a logistic regresion classifier.\n",
    "#Obtained AUC in test = 0.775\n",
    "\n",
    "#Best AUC reached : 0.77909119. running best 20 and selectiog 30.\n",
    "#Best AUC 0.78433477 running best 5 and selecting 18.\n",
    "#Best AUC 0.77989349 running best 5 and selecting 10.\n",
    "#Best AUC 0.78518753 running best 10 and selecting 25.\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "clf = LogisticRegression()\n",
    "pipeline = Pipeline([('scaler',scaler), ('model', clf)])\n",
    "grid_values = {'model__penalty': ['l1', 'l2'], 'model__C':[0.01, 0.1, 1, 10, 100]}\n",
    "search = GridSearchCV(pipeline, grid_values, cv =5, scoring = \"roc_auc\").fit(X,y)\n",
    "\n",
    "cv_result = search.cv_results_\n",
    "\n",
    "mean_train_score = cv_result['mean_train_score']\n",
    "\n",
    "mean_test_score = cv_result['mean_test_score']\n",
    "\n",
    "results_train = np.array(mean_train_score).reshape(5,2)\n",
    "\n",
    "results = np.array(mean_test_score).reshape(5,2)\n",
    "\n",
    "params = search.best_params_\n",
    "\n",
    "#For each combination of parameters, there is an average of the auc metric calculated within a logistic regression classifier.\n",
    "#L1,l2 as columns, and 0.01, 0.1, 0.1, 10, 100 as rows in the results matrix.\n",
    "print(cv_result)\n",
    "print('')\n",
    "print('train')\n",
    "print(results_train)\n",
    "print('')\n",
    "print('test')\n",
    "print(results)\n",
    "print('')\n",
    "print(params)\n",
    "print('')\n",
    "\n",
    "#Predict probabilities.\n",
    "y_prob_logistic_regression = search.predict_proba(X)\n",
    "\n",
    "print(y_prob_logistic_regression)\n",
    "\n",
    "#Get probabilities.\n",
    "y_prob_logistic_regression = get_probs_format(X,y_prob_logistic_regression)\n",
    "print(y_prob_logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train a randomforest classifier with randomsearch.\n",
    "#Obtained AUC in test = \n",
    "\n",
    "#The classifier is tried with randomsearch to check the best parameters combinations that make better classifications.\n",
    "#Crossvalidation is not performed with this classifier cause it is nessesary an X_test(validation) to evaluate the auc metric.\n",
    "#X = X[selected_features]\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "\n",
    "#The parameter class_weight='balanced_subsample' will check the proportion of labels of each class in each tree.\n",
    "#Based on that, it will calculate the metric in favor to the minority class.\n",
    "#clf = RandomForestClassifier(class_weight='balanced_subsample')\n",
    "\n",
    "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "#n_estimators = [200]\n",
    "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 210, num = 2)]\n",
    "# Number of features to consider at every split\n",
    "#max_features = ['auto']\n",
    "#max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "#max_depth = [10]\n",
    "#max_depth = [int(x) for x in np.linspace(10, 110, num = 10)]\n",
    "#max_depth = [int(x) for x in np.linspace(10, 20, num = 2)]\n",
    "#max_depth.append(None)\n",
    "#min_samples_split[2]\n",
    "# Minimum number of samples required to split a node\n",
    "#min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "#min_samples_leaf = [1]\n",
    "#min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True]\n",
    "#bootstrap = [True, False]\n",
    "\n",
    "#random_grid = {'n_estimators': n_estimators,\n",
    "#               'max_features': max_features,\n",
    "#               'max_depth': max_depth,\n",
    "#               'min_samples_split': min_samples_split,\n",
    "#               'min_samples_leaf': min_samples_leaf,\n",
    "#               'bootstrap': bootstrap}\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#n_iter tells us the number of different combinations to try.\n",
    "#rf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 5, cv = 5, verbose=2, random_state=42, n_jobs = -1, scoring =\"roc_auc\")# Fit the random search model\n",
    "\n",
    "#Training the classifier.\n",
    "#rf_random.fit(X_train, y_train)\n",
    "\n",
    "#Print results\n",
    "#auc_train = roc_auc_score(y_train, rf_random.predict(X_train))\n",
    "#auc_test = roc_auc_score(y_val, rf_random.predict(X_val))\n",
    "\n",
    "#Predict probabilities.\n",
    "#y_prob_random_forest_rs = grid_search.predict_proba(X_val)\n",
    "\n",
    "#print(rf_random.best_params_)\n",
    "#print(auc_train)\n",
    "#print(auc_test)\n",
    "\n",
    "#print(y_prob_random_forest_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train a randomforest classifier with gridsearch.\n",
    "#Obtained AUC in test = 0.66\n",
    "\n",
    "#Try a grid search classifier with gridsearch to see parameters combinations that make better classifications.\n",
    "#Crossvalidation is not performed with this classifier cause it is nessesary an X_test(validation) to evaluate the auc metric.\n",
    "#from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "#X = X[selected_features]\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "\n",
    "#The parameter class_weight='balanced_subsample' will check the proportion of labels of each class in each tree.\n",
    "#Based on that, it will calculate the metric in favor to the minority class.\n",
    "#clf = RandomForestClassifier(class_weight='balanced_subsample')\n",
    "\n",
    "#param_grid = {\n",
    "#    'bootstrap': [True],\n",
    "#    #'max_depth': [80, 90, 100, 110],\n",
    "#    'max_features': ['auto', 'sqrt'],\n",
    "#    'min_samples_leaf': [5,6],\n",
    "#    #'min_samples_split': [8, 10, 12],\n",
    "#    'n_estimators': [50, 75, 150]\n",
    "#}# Create a based model\n",
    "\n",
    "#grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "#Training the classifier.\n",
    "#grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Print results\n",
    "#auc_train = roc_auc_score(y_train, grid_search.predict(X_train))\n",
    "#auc_test = roc_auc_score(y_val, grid_search.predict(X_val))\n",
    "\n",
    "#Predict probabilities.\n",
    "#y_prob_random_forest_rs = grid_search.predict_proba(X_val)\n",
    "\n",
    "#print(grid_search.best_params_)\n",
    "#print(auc_train)\n",
    "#print(auc_test)\n",
    "\n",
    "#print(\"\")\n",
    "#print(y_prob_random_forest_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train a supporting vector machines classifier.\n",
    "#Obtained AUC in test = \n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "#X = X[selected_features]\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "\n",
    "#The parameter class_weight='balanced_subsample' will check the proportion of labels of each class in each tree.\n",
    "#Based on that, it will calculate the metric in favor to the minority class.\n",
    "#clf = SVC(class_weight='balanced', probability=True)\n",
    "\n",
    "#param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              #'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              #'kernel': ['rbf']} \n",
    "        \n",
    "#param_grid = {'C': [1], \n",
    "  #            'gamma': [1],\n",
    "  #            'kernel': ['rbf']}\n",
    "\n",
    "#grid_search = GridSearchCV(estimator = clf, param_grid = param_grid, \n",
    "                         # cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "#Training the classifier.\n",
    "#grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Print results\n",
    "#auc_train = roc_auc_score(y_train, grid_search.predict(X_train))\n",
    "#auc_test = roc_auc_score(y_val, grid_search.predict(X_val))\n",
    "\n",
    "#Predict probabilities\n",
    "#y_prob_SVM = grid_search.predict_proba(X_val)\n",
    "\n",
    "#print(grid_search.best_params_)\n",
    "#print(auc_train)\n",
    "#print(auc_test)\n",
    "\n",
    "#print(\"\")\n",
    "#print(y_prob_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try a gradient boosting classifier on the problem.\n",
    "#Obtained AUC in test = 0.623612649317\n",
    "\n",
    "#X = X[selected_features]\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.20)\n",
    "\n",
    "#The parameter class_weight='balanced_subsample' will check the proportion of labels of each class in each tree.\n",
    "#Based on that, it will calculate the metric in favor to the minority class.\n",
    "#clf = GradientBoostingClassifier()\n",
    "\n",
    "#Finding the optimal number of desicion trees.\n",
    "#errors = [mean_squared_error(y_test, y_pred) for y_pred in regressor.staged_predict(X_test)]\n",
    "#best_n_estimators = np.argmin(errors)\n",
    "\n",
    "#parameters = {\n",
    "#    \"learning_rate\": [0.01,0.05, 0.1],\n",
    "#    \"n_estimators\":[10,50,100],\n",
    "#    \"subsample\":[0.5, 0.8, 0.85]\n",
    "#}\n",
    "\n",
    "#parameters = {\n",
    "    #\"loss\":[\"deviance\"],\n",
    "    #\"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    #\"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    #\"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    #\"max_depth\":[3,5,8],\n",
    "    #\"max_features\":[\"log2\",\"sqrt\"],\n",
    "    #\"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    #\"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    #\"n_estimators\":[10]\n",
    "#}\n",
    "\n",
    "\n",
    "#grid_search = GridSearchCV(estimator = clf, param_grid = parameters, scoring = \"roc_auc\", cv = 5, n_jobs = -1, verbose = 2)\n",
    "\n",
    "#Training the classifier.\n",
    "#grid_search.fit(X_train, y_train)\n",
    "\n",
    "#Print results\n",
    "#auc_train = roc_auc_score(y_train, grid_search.predict(X_train))\n",
    "#auc_test = roc_auc_score(y_val, grid_search.predict(X_val))\n",
    "\n",
    "#print(grid_search.best_params_)\n",
    "#print(auc_train)\n",
    "#print(auc_test)\n",
    "\n",
    "#Predict probabilities\n",
    "#y_prob_GB = grid_search.predict_proba(X_val)\n",
    "\n",
    "#print(y_prob_GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>6. Model evaluation on unseen data </h3>\n",
    "<p>Evaluating how well the model performs on data that it hasn't seen before. The evaluation is performed over the X_test set.</p>\n",
    "<ul>\n",
    "<li>A Logistic Regression was selected as the best classifier to perform the prediction task for ticket compliance during the training and model selection phase </li>\n",
    "<li>At the final part, probabilites of the given predictions are calculated.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id\n",
      "284932    0.057680\n",
      "285362    0.020298\n",
      "285361    0.062121\n",
      "285338    0.055008\n",
      "285346    0.062120\n",
      "285345    0.055009\n",
      "285347    0.065990\n",
      "285342    0.481184\n",
      "285530    0.018669\n",
      "284989    0.036749\n",
      "285344    0.065990\n",
      "285343    0.020297\n",
      "285340    0.020297\n",
      "285341    0.065990\n",
      "285349    0.057326\n",
      "285348    0.050734\n",
      "284991    0.036749\n",
      "285532    0.038018\n",
      "285406    0.038014\n",
      "285001    0.035770\n",
      "285006    0.010449\n",
      "285405    0.018667\n",
      "285337    0.036759\n",
      "285496    0.069186\n",
      "285497    0.057706\n",
      "285378    0.019617\n",
      "285589    0.035016\n",
      "285585    0.055019\n",
      "285501    0.065143\n",
      "285581    0.020301\n",
      "            ...   \n",
      "376367    0.014764\n",
      "376366    0.050012\n",
      "376362    0.348011\n",
      "376363    0.236119\n",
      "376365    0.014764\n",
      "376364    0.050012\n",
      "376228    0.052465\n",
      "376265    0.052467\n",
      "376286    0.293951\n",
      "376320    0.050010\n",
      "376314    0.050010\n",
      "376327    0.283567\n",
      "376385    0.283577\n",
      "376435    0.465377\n",
      "376370    0.496949\n",
      "376434    0.074343\n",
      "376459    0.070021\n",
      "376478    0.000134\n",
      "376473    0.052476\n",
      "376484    0.027414\n",
      "376482    0.019597\n",
      "376480    0.019597\n",
      "376479    0.019597\n",
      "376481    0.019597\n",
      "376483    0.026863\n",
      "376496    0.011286\n",
      "376497    0.011286\n",
      "376499    0.070023\n",
      "376500    0.070023\n",
      "369851    0.078346\n",
      "Name: compliance, dtype: float64\n",
      "\n",
      "(61001,)\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression prediction over the test set.\n",
    "#Predict probabilities.\n",
    "y_prob_logistic_regression = search.predict_proba(X_test)\n",
    "y_prob_logistic_regression = get_probs_format(X_test, y_prob_logistic_regression)\n",
    "print(y_prob_logistic_regression)\n",
    "print(\"\")\n",
    "print(y_prob_logistic_regression.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blight_model():\n",
    "    \n",
    "    answer = y_prob_logistic_regression\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id\n",
       "284932    0.057680\n",
       "285362    0.020298\n",
       "285361    0.062121\n",
       "285338    0.055008\n",
       "285346    0.062120\n",
       "285345    0.055009\n",
       "285347    0.065990\n",
       "285342    0.481184\n",
       "285530    0.018669\n",
       "284989    0.036749\n",
       "285344    0.065990\n",
       "285343    0.020297\n",
       "285340    0.020297\n",
       "285341    0.065990\n",
       "285349    0.057326\n",
       "285348    0.050734\n",
       "284991    0.036749\n",
       "285532    0.038018\n",
       "285406    0.038014\n",
       "285001    0.035770\n",
       "285006    0.010449\n",
       "285405    0.018667\n",
       "285337    0.036759\n",
       "285496    0.069186\n",
       "285497    0.057706\n",
       "285378    0.019617\n",
       "285589    0.035016\n",
       "285585    0.055019\n",
       "285501    0.065143\n",
       "285581    0.020301\n",
       "            ...   \n",
       "376367    0.014764\n",
       "376366    0.050012\n",
       "376362    0.348011\n",
       "376363    0.236119\n",
       "376365    0.014764\n",
       "376364    0.050012\n",
       "376228    0.052465\n",
       "376265    0.052467\n",
       "376286    0.293951\n",
       "376320    0.050010\n",
       "376314    0.050010\n",
       "376327    0.283567\n",
       "376385    0.283577\n",
       "376435    0.465377\n",
       "376370    0.496949\n",
       "376434    0.074343\n",
       "376459    0.070021\n",
       "376478    0.000134\n",
       "376473    0.052476\n",
       "376484    0.027414\n",
       "376482    0.019597\n",
       "376480    0.019597\n",
       "376479    0.019597\n",
       "376481    0.019597\n",
       "376483    0.026863\n",
       "376496    0.011286\n",
       "376497    0.011286\n",
       "376499    0.070023\n",
       "376500    0.070023\n",
       "369851    0.078346\n",
       "Name: compliance, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blight_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
